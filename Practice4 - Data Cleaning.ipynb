#!/usr/bin/env python
# coding: utf-8

# # Data Cleaning in Python
# The examples and code in this notebook are taken from the [Kaggle Data Cleaning Challenge](https://www.kaggle.com/getting-started/52652)
# 
# Detailed explanations for important code snippets are provided by Mervat Abuelkheir as part of the CSEN1095 Data Engineering Course.
# 
# The goal of this notebook is to show how to handle missing values and noisy data.
# 
# Pay attention to the <span style="color:red"> <b> paragraphs in bold red</b></span>; they ask you to do something and provide input!
# 

# ## Inspecting the data
# 
# First thing we need to do is import a dataset.
# 
# <br>We will work in this exercise with the <a href="https://www.kaggle.com/maxhorowitz/nflplaybyplay2009to2016#NFL%20Play%20by%20Play%202009-2017%20(v4).csv"> NFL dataset</a>. The NFL dataset is available in the repository's data folder, and is made available on Kaggle. It contains all the regular season plays from the 2009-2016 NFL seasons. The dataset has 100 columns. Each play contains information on: game situation, players involved, results, and advanced metrics such as expected point and win probability values.
# 
# ### Important note
# 
# <b>The NFL dataset is very big (200+ MB) and is not made available in the `data` folder in this repository. Download the dataset from the link provided, store it in your own data or working directory, and proceed with running the code below, taking care to change your directory according to your own data storage location.</b>
# 
# <b>For large datasets, you will be provided with a link through which you can download the data. Another option is to read from the html link, provided the data is well-organized to help with proper parsing.</b>
# 

# In[2]:


# modules we'll use
import pandas as pd
import numpy as np

# read in all our data
nfl_data = pd.read_csv("data/NFL Play by Play 2009-2017 (v4).csv")

# set seed for reproducibility
np.random.seed(0) 


# In[4]:


# Look at a few rows of the nfl_data file. A handful of missing data!
nfl_data.sample(4)


# ## See how many missing data points we have
# 
# Let's see how many we have in each column. Let's start with the NFL dataset. 

# In[5]:


# get the number of missing data points per column
missing_values_count_nfl = nfl_data.isnull().sum()

# look at the # of missing points in the first ten columns
print(missing_values_count_nfl)


# There are a lot of missing values it seems! Let's take a look at the percentage of the missing values in the dataset.

# In[6]:


# How many total missing values do we have?
# shape returns the dimentionality of a dataframe (rows and columns), can you guess what product will do?
total_cells_nfl = np.product(nfl_data.shape) 
total_missing_nfl = missing_values_count_nfl.sum()

# percent of data that is missing
percentage_missign_values_nfl = (total_missing_nfl/total_cells_nfl) * 100
print(percentage_missign_values_nfl)


# Almost a quarter of the cells in this dataset are empty! 
# 
# In the next step, we're going to take a closer look at some of the columns with missing values and try to figure out what might be going on with them.

# ## Examining the missing values
# 
# This is the point where you should develop some data intuition and look at the data to try and figure out why it is the way it is and how that will affect your analysis. For dealing with missing values, you'll need to use your intution to figure out why the value is missing. One of the most important question you can ask yourself to help figure this out is this:
# 
# **Is this value missing becuase it wasn't recorded or becuase it dosen't exist?**
# 
# If a value is missing becuase it doens't exist (like the height of the oldest child of someone who doesn't have any children) then it doesn't make sense to try and guess what it might be. These values you probably do want to keep as `NaN`. On the other hand, if a value is missing becuase it wasn't recorded, then you can try to guess what it might have been based on the other values in that column and row. (This is called "imputation" and we'll learn how to do it next! :)
# 
# Let's work through an example. Looking at the number of missing values in the nfl_data dataframe, notice that the column TimesSec has a lot of missing values in it: 

# In[7]:


# look at the # of missing points in the first ten columns
missing_values_count_nfl[0:10]


# The `TimeSecs` column has information on the number of seconds left in the game when the play was made. This means that these values are probably missing because they were not recorded, rather than because they don't exist. So, it would make sense for us to try and guess what they should be rather than just leaving them as NA's.
# 
# On the other hand, there are other fields, like `PenalizedTeam` that also have lot of missing fields. In this case, though, the field is missing because if there was no penalty then it doesn't make sense to say which team was penalized. For this column, it would make more sense to either leave it empty or to add a third value like "neither" and use that to replace the NA's.
# 
# **Tip: This is a great place to read over the dataset documentation if you haven't already! If you're working with a dataset that you've gotten from another person, you can also try reaching out to them to get more information.**
# 
# You should look at each column individually to figure out the best strategy for filling those missing values. For the rest of this notebook, we'll cover some "quick and dirty" techniques that can help you with missing values but will probably also end up removing some useful information or adding some noise to your data.

# ## Dropping the missing values
# 
# If you don't have a reason to figure out why your values are missing, one option you have is to just remove any rows or columns that contain missing values. (Note: DO NOT do this approch for important projects! It's usually worth it to take the time to go through your data and really look at all the columns with missing values one-by-one to really get to know your dataset.)  
# 
# If you're sure you want to drop rows with missing values, pandas does have a handy function, `dropna()` to help you do this. Let's try it out on our NFL dataset!

# In[8]:


# remove all the rows that contain a missing value
nfl_data.dropna()


# Oops! That's removed all our data! <span style="color:red"><b>Why do you think this happened?</b></span>
# 
# We might have better luck removing all the columns that have at least one missing value instead.

# In[9]:


# Remove all columns with at least one missing value
columns_with_na_dropped = nfl_data.dropna(axis=1) # axis 1 indicates operation along columns
columns_with_na_dropped.head()


# Let's check how much data we lost.

# In[10]:


print("Columns in original dataset: %d \n" % nfl_data.shape[1])
print("Columns with na's dropped: %d" % columns_with_na_dropped.shape[1])


# We've lost quite a bit of data, but at this point we have successfully removed all the `NaN`'s from our data. 

# # Filling in missing values automatically
# 
# Another option is to try and fill in the missing values. Let's get a small sub-section of the NFL data so that it will print well and work with that to avoid having to work with the entire dataset and consume CPU time. You can work on the entire dataset at home!

# In[11]:


# get a small subset of the NFL dataset
subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()
subset_nfl_data


# We can use the Panda's fillna() function to fill in missing values in a dataframe for us. One option we have is to specify what we want the `NaN` values to be replaced with. Here, I'm saying that I would like to replace all the `NaN` values with 0.

# In[12]:


# replace all NA's with 0
subset_nfl_data.fillna(0) 


# We could also be a bit more savvy and replace missing values with whatever value comes directly after it in the same column. (This makes a lot of sense for datasets where the observations have some sort of logical order to them.)

# In[13]:


# replace all NA's the value that comes directly after it in the same column, 
# then replace all the reamining na's with 0
subset_nfl_data.fillna(method = 'bfill', axis=0).fillna(0) # here you may want to explore other methods like 'mean' if it makes sense


# ## Trying multivariate imputation with sklearn
# 
# sklearn has an `Imputer` and a `SimpleImputer` modules which perform imputation on one column at a time using the mean, median, or mode. 
# 
# sklearn also has a multivariate imputation module, `IterativeImputer`, that can help us impute values across multiple columns, but it is experimental and only available through developer release.
# 
# Let's see how an sklearn's imputation works for the NFL dataset.

# In[14]:


from sklearn.preprocessing import Imputer
#from sklearn.impute import SimpleImputer # pretty much the same thing, you can try yourself
mean_imputer = Imputer(missing_values=np.nan, strategy='mean')
#mean_simpleimputer = SimpleImputer(missing_values=np.nan, strategy='mean') # Same use for sklearn SimpleImputer

# Again, let's get a small subset of the NFL dataset
subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()

# Check the presence of null values in the subset chosen
subset_nfl_data.isnull().sum()


# In[15]:


# Now let's impute missing values in yacWPA column with the mean
mean_imputer.fit(subset_nfl_data[["yacWPA"]])
subset_nfl_data[["yacWPA"]]=mean_imputer.transform(subset_nfl_data[["yacWPA"]]).ravel()
#imputed_subset_nfl_data = pd.DataFrame(mean_simpleimputer.fit_transform(subset_nfl_data), columns=subset_nfl_data.columns) # fitting and transforming can be done in one step

# Let's check the presence null values again to see if imputation worked for the column
subset_nfl_data.isnull().sum()
# add you own counting of nulls for the dataframe imputed by SimpleImputer


# In[16]:


# Print the dataframe 
print(subset_nfl_data)
#print(imputed_subset_nfl_data) # if you want to try SimpleImputer code
# You may want to try the same code snippets above with the entire dataset and not just the subset


# Now let's try the more advanced `IterativeImputer` in sklearn. It provides means to do multivariate imputation using a variety of ML models.

# In[17]:


# This code snippet may not work, so do not worry about it, and just examine the similarities with the SimpleImputer
# To use this experimental feature, we need to explicitly ask for it:
from sklearn.experimental import enable_iterative_imputer  
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge # Imputation via linear regression
from sklearn.neighbors import KNeighborsRegressor # Imputation via KNN
# Define imputer
reg_imputer = IterativeImputer(BayesianRidge, max_iter=5, random_state=0)
subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()
imputed_subset_nfl_data = pd.DataFrame(mean_simpleimputer.fit_transform(subset_nfl_data), columns=subset_nfl_data.columns)
print(imputed_subset_nfl_data)


# ## <span style="color:red"> Exercise 1: Missing value handling on the San Francisco Building Permits Dataset </span>
# 
# <span style="color:red"><b> The <a href="https://www.kaggle.com/aparnashastry/building-permit-applications-data#Building_Permits.csv"> Building permits dataset</a> pertains to all types of structural permits from Jan 1, 2013-Feb 25th 2018 in San Francisco. Data includes details on application/permit numbers, job addresses, supervisorial districts, and the current status of the applications. A data dictionary for the dataset is available in the data folder in this repository.</b></span>
# 
# <span style="color:red"> <b>The dataset is rather big and does not exist in the "data" folder in this repository. Download the dataset from the link provided and store it in your own data or working folder. Import the dataset in the next cell, and work your way through the steps in the subsequent cells.</b></span>

# In[19]:


# modules we'll use
import pandas as pd
import numpy as np

# read in all our data
data = pd.read_csv("data/Building_Permits.csv")
data.head(10)


# <span style="color:red"> <b>Find out what percent of the building permits dataset is missing. </b></span>
# <span style="color:red"> <b><br>Write down your code in the following cell. </b></span>

# In[20]:


# get the number of missing data points per column
missing_values_count_nfl = data.isnull().sum()
total_cells_nfl = np.product(data.shape) 
total_missing_nfl = missing_values_count_nfl.sum()

# percent of data that is missing
percentage_missign_values_nfl = (total_missing_nfl/total_cells_nfl) * 100
print(percentage_missign_values_nfl)


# <span style="color:red"><b>Write a code to check the values of the `Street Number Suffix` and `Zipcode` from the building permits datasets. Both of these contain missing values. Which, if either, of these are missing because they don't exist? Which, if either, are missing because they weren't recorded?</b></span>

# In[21]:


print(missing_values_count_nfl[9:10])
print(missing_values_count_nfl[40:41])


# <span style="color:red"><b>See if you can drop the missing values from the building permits dataset and still keep some data, or drop the columns with missing values and see if you have columns still.</b></span>

# In[23]:


columns_with_na_dropped = data.dropna(axis=1) # axis 1 indicates operation along columns
columns_with_na_dropped.head(10)


# <span style="color:red"><b>Try replacing all the `NaN`'s in the building permits data with the one that comes directly after it and then replacing any remaining `NaN`'s with 0.</b></span>

# In[24]:


columns_with_na_dropped.fillna(method = 'bfill', axis=0).fillna(0)


# <span style="color:red"><b>Create a new dataset at the ending with all missing values handled and store as a CSV file.</b></span>

# In[26]:


columns_with_na_dropped.to_csv(r"data/Building_PermitsNew.csv", sep='\t', encoding='utf-8',index=False)


# ## <span style="color:red"> Exercise 2: Missing value handling on the  Pima Indians Diabetes Dataset</span>
# 
# <span style="color:red"><b> The <a href= "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"> Pima Indians Diabetes Dataset</a> involves predicting the onset of diabetes within 5 years in Pima Indians given medical details.</b></span>
# 
# <span style="color:red">There are 768 observations with 8 input variables and 1 output variable. The variable names are as follows:</span>
# 
# <br><span style="color:red">0. Number of times pregnant.</span>
# <br><span style="color:red">1. Plasma glucose concentration a 2 hours in an oral glucose tolerance test.</span>
# <br><span style="color:red">2. Diastolic blood pressure (mm Hg).</span>
# <br><span style="color:red">3. Triceps skinfold thickness (mm).</span>
# <br><span style="color:red">4. 2-Hour serum insulin (mu U/ml).</span>
# <br><span style="color:red">5. Body mass index (weight in kg/(height in m)^2).</span>
# <br><span style="color:red">6. Diabetes pedigree function.</span>
# <br><span style="color:red">7. Age (years).
# <br><span style="color:red">8. Class variable (0 or 1).</span>
# 
# <span style="color:red"> <b>This is a medical dataset, so an important thing we will do with this dataset is that we will see if the statistical summaries can reveal something about the missing values.</b></span>
# 
# <span style="color:red"> <b>The dataset is available in the `data` folder in this repository. Import the dataset in the next cell, and work your way through the steps in the subsequent cells.</b></span>

# In[11]:


# modules we'll use
import pandas as pd
import numpy as np
import re

data = pd.read_csv("data/pima-indians-diabetes.data.csv")

# set seed for reproducibility
np.random.seed(0)
data.head(10)


# <span style="color:red"> <b>Use the describe method used for pandas data frames to obtain a statistucal summary of the dataset.</b></span>

# In[12]:


data.dropna(inplace = True)  
  
# percentile list 
perc =[.20, .40, .60, .80] 
  
# list of dtypes to include 
include =['object', 'float', 'int'] 
  
# calling describe method 
desc = data.describe(percentiles = perc, include = include) 
  
# display 
desc 


# <span style="color:red"><b>There are columns that have a minimum value of zero (0). On some columns, a value of zero does not make sense and indicates an invalid or missing value. For example, the `BMI` for a person cannot be 0. See if there are other columns/attributes whose value of 0 does not make sense. An understanding of the medical columns may be needed.</b></span>

# In[13]:


mask =(data["33.6"]==0)
data.loc[mask,"33.6"]=data["33.6"].mean()
data


# In[14]:


mask =(data["50"]==0)
data.loc[mask,"50"]=data["50"].mean()
data


# <span style="color:red"><b>Now you can proceed to check the missing values and remove/impute as in exercise 1. Create a new dataset at the ending with all missing values handled and store as a CSV file.</b></span>

# In[15]:


data.to_csv(r"data/pima-indians-diabetes.dataNew.csv", sep='\t', encoding='utf-8',index=False)


# In[ ]:




